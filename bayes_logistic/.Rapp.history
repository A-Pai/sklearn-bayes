colSdevs
x - colMeans(x)
colMeans(x - colMeans(x))
x[,1] - mean(x[,1])
mean(x[,1] - mean(x[,1]))
X = matrix(1:10, ncol = 2)
X
X - colMeans(X)
colMeans(X - colMeans(X))
xd = X - colMeans(X)
colMeans(x)
colMeans(X)
X - t(colMeans(X))
rep(colMeans(X), times = 5)
help(scale)
scale(X)
colMeans(X)
colMeans(scale(X))
sd(scale(X)[,1])
diag(1)
diag(1, nrow = 2)
Diag = diag(1, nrow = 2)
Diag[1,1]
Diag[1,1] = 0
x
c = 1:10
c
c[1:]
c[1:4]
c[1]
c[1:length(x)]
c[1:length(c)]
help(scale)
help(scale)
X
sapply(1:2, function(i){sd(X[,i])})
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'numeric',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  ==  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  ==  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% t( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,x){ standardGeneric('predict')})#
    setMethod('predict', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX        = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]    = rep(1,times = n)#
              		newX[,2:]   = X#
              		X           = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'numeric',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  ==  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  ==  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% t( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,x){ standardGeneric('predict')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX        = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]    = rep(1,times = n)#
              		newX[,2:]   = X#
              		X           = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'numeric',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  ==  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  ==  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% t( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,x){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX        = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]    = rep(1,times = n)#
              		newX[,2:]   = X#
              		X           = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'numeric',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  ==  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  ==  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% t( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,x){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'numeric',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  ==  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  ==  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% t( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
br = BayesianLogisticRegression()
br
X = matrix(rnorm(100), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1
X = matrix(rnorm(100), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
br
X = matrix(rnorm(100), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
br@N
X
Y
X
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
X
plot(X)
plot(X,'red')
plot(X,col = 'red')
plot(X,col = 'red', type = 20)
plot(X,col = 'red', type = 'rb')
plot(X,col = 'red')
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
print 2
print(2)
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'numeric',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  ==  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  ==  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	print (theObject@N)#
              	print (theObject@M)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% t( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
dim(X)
dim(X)[1]
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'numeric',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	print (theObject@N)#
              	print (theObject@M)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% t( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	print (theObject@N)#
              	print (theObject@M)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( X %*% ( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
getwd()
help(load)
setwd('/Users/amazaspshaumyan/Desktop/Bayesian Regression Methods/Bayesian Logistic Regression/')
dir()
load(dir()[5])
print (dim(X))#
              	print (dim(Y))
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	print (dim(X))#
              	print (dim(Y))#
              	XY   = colSums( X %*% ( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
Y
dim(Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	print (dim(X))#
              	print (dim(Y))#
              	XY   = colSums( X %*% ( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	print (dim(X))#
              	print (dim(Y))#
              	XY   = colSums( t(X) %*% ( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( t(X) %*% ( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( t(X) %*% ( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		print length(XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
print (X)
print X
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = colSums( t(X) %*% ( Y - 0.5 ) )#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
help(colSums)
X
colSums(X)
dim(colSums(X))
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( colSums( t(X) %*% ( Y - 0.5 ) ), ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) ), ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		XM   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + trace(Sn) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
help(trace)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-6,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
br
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (dim(Sn))#
              		print (dim(XY))#
              		print (XY)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (XY)#
              		print (Sn)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(Sn.inv)#
              		# mean update#
              		print (XY)#
              		print (Sn)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
X
help(chol2inv)
inv
help(inv)
help(chol2inv)
help(solve)
x = matrix(runif(10),ncol = 2)
X = t(x)%*%x + diag(1,2)
X
solve(X)
solve(X) %*% X
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = solve(Sn.inv)#
              		# mean update#
              		print (XY)#
              		print (Sn)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(chol(Sn.inv))#
              		# mean update#
              		print (XY)#
              		print (Sn)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = chol2inv(chol(Sn.inv))#
              		# mean update#
              		print (XY)#
              		print (Sn)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression()#
fit(br,X,Y)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = FALSE)#
fit(br,X,Y)
help(qr.solve)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print (XY)#
              		print (Sn)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 5#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = FALSE)#
fit(br,X,Y)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = FALSE)#
fit(br,X,Y)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 20#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = FALSE)#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 20#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = FALSE)#
fit(br,X,Y)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 20#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (Xm)#
              		print (XSX)#
              		print (eps)#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 20#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(x){#
	# helper function used for local variational bound calculation#
	return (-0.5 / x * ( sigmoid(x) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		XSX  = rowSums(X %*% Sn * X)#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (XSX)#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 20#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (XSX)#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 20#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
X
help(det)
plot(X)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
plot(X)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = runif(N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}
})
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = 1#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
lambda(rep(1,times = 10))
lambda(rep(10,times = 10))
lambda(rep(100,times = 10))
lambda(0.1)
sigmoid(0.1)
sigmoid(0)
sigmoid(0.1)
(sigmoid(0.1) - 0.5)*(-0.5/0.1)
(sigmoid(0.1) - 0.5)
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(eps){#
	# helper function used for local variational bound calculation#
	return (0.5 / eps * ( sigmoid(eps) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
predict.probs(X)
X
predict.probs(X)
# @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probs', def = function(theObject,X){ standardGeneric('predict.probs')})#
    setMethod('predict.probs', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
predict.probs(X)
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(eps){#
	# helper function used for local variational bound calculation#
	return (0.5 / eps * ( sigmoid(eps) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probability', def = function(theObject,X){ standardGeneric('predict.probability')})#
    setMethod('predict.probability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
predict.probs(X)
predict.probability(X)
# @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probability', def = function(theObject,X){ standardGeneric('predict.probability')})#
    setMethod('predict.probability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
predict.probability(X)
setGeneric('predictprobability', def = function(theObject,X){ standardGeneric('predictprobability')})#
    setMethod('predictprobability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
predict.probability(X)
predictprobability(X)
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(eps){#
	# helper function used for local variational bound calculation#
	return (0.5 / eps * ( sigmoid(eps) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		if ( i== theObject@max.iter-1){#
              			theObject@coefs     = Mn#
              			theObject@coefs.cov = Sn#
              		}#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probability', def = function(theObject,X){ standardGeneric('predict.probability')})#
    setMethod('predict.probability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
predict.probability(X)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 2#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
plot(X)
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 4#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
plot(X)
model <- function(X,w){ -1*w[1]/w[3] - w[2]/w[3] * X[,1]}
br
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 4#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
y_hat = model(X,c(-0.2724551,3.9258915,4.8996411))
plot(y_hat)
model
X
help(seq)
model <- function(X,w){ -1*w[1]/w[3] - w[2]/w[3] * X}
x = seq(from = -1, to = 5, length.out = 200)
y_hat = model(x,c(-0.2724551,3.9258915,4.8996411))
plot(y_hat)
plot(y_hat)
lines(X)
lines(X,type = 'p')
plot(y_hat)
lines(X,type = 'p')
lines(X[,1],X[,2],type = 'p')
plot(X[,1],X[,2],type = 'p')
plot(X[,1],X[,2],type = 'p20')
plot(X[,1],X[,2],type = 'p',col = 'red')
lines(x,y_hat,col = 'blue')
br
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(eps){#
	# helper function used for local variational bound calculation#
	return (0.5 / eps * ( sigmoid(eps) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		theObject@coefs     = Mn#
              		theObject@coefs.cov = Sn#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probability', def = function(theObject,X){ standardGeneric('predict.probability')})#
    setMethod('predict.probability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 4#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
br
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		theObject@coefs     = matrix(Mn, ncol = 1)#
              		theObject@coefs.cov = matrix(Sn, ncol = M)#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 4#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
br
br@coefs
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(eps){#
	# helper function used for local variational bound calculation#
	return (0.5 / eps * ( sigmoid(eps) - 0.5 ) )#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0,#
                      coefs       = matrix(NA,ncol = 1)#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		theObject@coefs     = matrix(Mn, ncol = 1)#
              		theObject@coefs.cov = matrix(Sn, ncol = M)#
              	}#
#
              })#
    # @Method Name : predict#
    ##
    # @Description : predicts target value for explanatory variables#
    ##
    # @Parameters  :#
    # ==============#
    ##
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    setGeneric('predict.probability', def = function(theObject,X){ standardGeneric('predict.probability')})#
    setMethod('predict.probability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 4#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
fit(br,X,Y)
br
br@coefs = matrix(1:10,ncol = 1)
br
# @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		print ("XY,Sn,Sn.inv")#
              		print (XY)#
              		print (Sn)#
              		print (Sn.inv)#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		print("EPS,XSX,XM")#
              		print (det(Sn))#
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		print(Mn)#
              		# check convergence#
              		theObject@coefs     = matrix(Mn, ncol = 1)#
              		theObject@coefs.cov = matrix(Sn, ncol = M)#
              		return (theObject)#
              	}#
#
              })
X = matrix(rnorm(200), ncol = 2)#
X[1:50,] = X[1:50,] + 4#
Y = rep(0,times = 100)#
Y[1:50] = 1#
br = BayesianLogisticRegression(scale = TRUE)#
br = fit(br,X,Y)
br
predict.probability(br,X)
setGeneric('predict.probability', def = function(theObject,X){ standardGeneric('predict.probability')})#
    setMethod('predict.probability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m+1, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@Mn )#
              	return (probs)#
              })
predict.probability(br,X)
setGeneric('predict.probability', def = function(theObject,X){ standardGeneric('predict.probability')})#
    setMethod('predict.probability', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# dimensionality of X#
              	n = dim(X)[1]#
              	m = dim(X)[2]#
              	# scale test data if required#
              	if ( theObject@scale ){#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term if required#
              	if ( theObject@bias.term ){#
              		newX                 = matrix(data = NA, ncol = m+1, nrow = n)#
              		newX[,1]             = rep(1,times = n)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	probs = sigmoid( X %*% theObject@coefs )#
              	return (probs)#
              })
predict.probability(br,X)
pr = predict.probability(br,X)
pr
pr > 0.5
Y = c('a','b','a','b')
factor(Y)
y = factor(Y)
y
levels(y)
help(logdet)
help(det)
# ========================== Helper Functions ==================================#
#
sigmoid <- function(x) {#
	# Calculates value of sigmoid function#
	return ( 1.0 / ( 1 + exp(-x)) )#
}#
#
lambda <- function(eps){#
	# helper function used for local variational bound calculation#
	return (0.5 / eps * ( sigmoid(eps) - 0.5 ) )#
}#
#
preprocess <- function(X,bias,scale,muX,sdX){#
	# dimensionality of data#
	n = dim(X)[1]#
    m = dim(X)[2]#
	# scale data if required#
	if ( scale ){#
		X = scale(X, center = muX, scale = theObject@sdX)#
    }#
    # add bias term if required#
    if ( bias ){#
        newX      = matrix(data = NA, ncol = m+1, nrow = n)#
        newX[,1]  = rep(1,times = n)#
        newX[,2:m+1] = X#
        X                    = newX#
    }#
	return (X)#
}#
#
% ======================= Bayesian Logistic Regression =========================#
BayesianLogisticRegression <- setClass(#
    # Bayesian logistic regression with local variational bounds.#
    # Similar to standard Bayesian Logistic Regression, but uses local #
    # Jaakola-Jordan variational bound instead of Laplace approximation#
    ##
    # @Parameters:#
    # ============#
    # bias.term  : logical vector of size (1,1) [ DEFAULT = TRUE ]#
    #              If True adds columns of ones to matrix of explanatory variables#
    ##
    # max.iter   : numeric vector of size (1,1)  [ DEFAULT = 100 ]#
    #              Maximum number of iterations before convergence#
    ##
    # conv.thresh: numeric vector of size (1,1) [ DEFAULT = 1e-3 ]#
    #              Threshold for convergence of algorithm#
    #              #
    # w.mean0    : numeric vector of size ( number of features, 1)#
    #              Mean of prior distribution for coefficients#
    ##
    # w.prec0    : matrix of size (number of features, number of features)#
    #              Precision of prior distribution for coefficients#
    ##
    #           #
    # @References:#
    # ============#
    # 1) Bishop 2006, Pattern Recognition and Machine Learning ( Chapter 10 )#
    # 2) Jaakola & Jordan 1994, #
    # 3) Murphy 2012, Machine Learning A Probabilistic Perspective#
    # 4) Barber 2015, Bayesian Reasoning and Machine Learning ()#
    # ----------------- set name of the class ------------------------#
    "BayesianLogisticRegression",#
    # -----------------  define instance variables -------------------#
    slots = list(#
                  bias.term   = 'logical',#
                  max.iter    = 'numeric',#
                  conv.thresh = 'numeric',#
                  scale       = 'logical',#
                  muX         = 'numeric', # vector of column means#
                  sdX         = 'numeric', # vector of column standard devs#
                  w.mean0     = 'numeric', #
                  w.prec0     = 'numeric',#
                  coefs       = 'matrix',#
                  coefs.cov   = 'matrix',#
                  N           = 'numeric',#
                  M           = 'numeric'#
                 ),#
    # ------------- default values for instance variables -------------#
    prototype = list(#
                      bias.term   = TRUE,#
                      max.iter    = 100,#
                      conv.thresh = 1e-5,#
                      scale       = TRUE,#
                      w.prec0     = 1e-3,#
                      N           = 0,#
                      M           = 0,#
                      muX         = 0,#
                      sdX         = 0,#
                      coefs       = matrix(NA,ncol = 1)#
                     ),#
    )#
    # ----------------------- define methods --------------------------#
    # @Method Name : fit#
    # #
    # @Description : Fits Bayesian Logistic Regression#
    ##
    # @Parameters  : #
    # ==============#
    # X: matrix of dimensionality (number of samples, number of features)#
    #     Matrix of explanatory variables#
    ##
    # Y: numeric vector of dimensionality (number of samples, 1)#
    #     Vector of dependent variables#
    ##
    ##
    setGeneric( 'fit', def = function(theObject,X,Y){ standardGeneric('fit') } )#
    setMethod('fit',signature = c('BayesianLogisticRegression','matrix','numeric'), #
              definition = function(theObject,X,Y){#
              	# check whether dimensionality is correct, if wrong change it#
              	if ( dim(X)[1]  !=  theObject@N ) theObject@N = dim(X)[1]#
              	if ( dim(X)[2]  !=  theObject@M ) theObject@M = dim(X)[2]#
              	N               =   theObject@N#
              	# transform Y into matrix to have conformable arguments for inner product #
                Y = matrix(Y, ncol = 1)#
              	# scale X for better convergence if necessary#
              	if ( theObject@scale ) {#
              		theObject@muX = colMeans(X)#
              		theObject@sdX = sapply(1:theObject@M, function(i){sd(X[,i])})#
              		X = scale(X, center = theObject@muX, scale = theObject@sdX)#
              	}#
              	# add bias term to matrix of explanatory variables if required#
              	if ( theObject@bias.term) {#
              		theObject@M          = theObject@M + 1#
             		newX                 = matrix(data = NA, ncol = theObject@M, nrow = N)#
              		newX[,1]             = rep(1,times = N)#
              		newX[,2:theObject@M] = X#
              		X                    = newX#
              	}#
              	M               = theObject@M#
              	# mean , precision and variational parameters#
              	w.mean0        = rep(0, times = M)#
              	alpha          = theObject@w.prec0#
              	eps            = rep(1,times = N)#
              	# precompute some values before#
              	XY   = matrix( t(X) %*% ( Y - 0.5 ) , ncol = 1)#
              	# iterations of EM algorithm#
              	for( i in 1:theObject@max.iter){#
              		# E-step : find parameters of posterior distribution of coefficients#
              		#          1) covariance of posterior#
              		#          2) mean of posterior#
              		# covariance update#
              		Xw        = X * matrix( rep(lambda(eps),times = M), ncol = M)   #
              		XXw       = t(X) %*% Xw#
              		# do not 'regularise' constant !!!#
              		Diag      = diag(alpha, nrow = M)#
              		#Diag[1,1] = 0#
              		Sn.inv    = 2*XXw + Diag#
              		# Sn.inv is positive definite due to adding of positive diagonal#
              		# hence Cholesky decomposition (that is used for inversion) should be stable#
              		Sn       = qr.solve(Sn.inv, tol = 1e-7)#
              		# mean update#
              		Mn       = Sn %*% XY#
              		# M-step : 1) update variational parameter eps for each observation#
              		#          2) update precision parameter (alpha)#
              		# variational parameter update#
              		Xm   = (X %*% Mn)^2#
              		#XSX  = rowSums(X %*% Sn * X)#
              		XSX  = diag(X %*% Sn %*% t(X))#
              		eps  = sqrt( Xm + XSX ) #
              		# update of precision parameter for coefficients (except for )#
              		alpha = M / ( sum(Mn[2:M]^2) + sum(diag((Sn))) - Sn[1,1] )#
              		# check convergence ()#
              		if #
              		theObject@coefs     = matrix(Mn, ncol = 1)#
              		theObject@coefs.cov = matrix(Sn, ncol = M)#
              		return (theObject)#
              	}#
#
              })#
    # @Method Name : predict.probs.fast#
    ##
    # @Description : Fast but not very accurate way to predict probability for explanatory variables#
    #                Use only in case speed is essential, otherwise use predict.probs#
    ##
    # @Parameters  :#
    # ==============#
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    # @Returns:#
    # =========#
    # probs: numeric vector of size ( number of samples in test set, 1 )  #
    #    Vector of probabilities#
    # #
    setGeneric('predict.probs.fast', def = function(theObject,X){ standardGeneric('predict.probs.fast')})#
    setMethod('predict.probs.fast', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# preprocess data#
              	X = preprocess(X,theObject@bias.term,theObject@scale,theObject@muX,theObject@sdX)#
              	# calculate & return probabilities#
              	probs = sigmoid( X %*% theObject@coefs )#
              	return (probs)#
              })#
    # @Method Name : predict#
    # #
    # @Description : predicts value of target#
    ##
    # @Parameters  :#
    # ==============#
    # X: matrix of size  (number of samples in test set, number of features)#
    #    Matrix of explanatory variables#
    ##
    # @Returns:#
    # =========#
    # targets: vector of size ( number of samples in test set, 1 )  #
    #    Vector of predicted targets#
    # #
    setGeneric('predict', def = function(theObject,X){ standardGeneric('predict') })#
    setMethod('predict', signature = c('BayesianLogisticRegression','matrix'), #
              definition = function(theObject,X){#
              	# preprocess data#
              	X = preprocess(X,theObject@bias.term,theObject@scale,theObject@muX,theObject@sdX)#
              })
