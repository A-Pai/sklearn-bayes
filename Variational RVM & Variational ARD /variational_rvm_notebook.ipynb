{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Relevance Vector Machine: Comparison of VRVR, RVR and SVR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this small tutorial we compare Variational Relevance Vector Machine, Type II ML Relevance Vector Machine and SVM.\n",
    "Main difference between standard RVR and VRVR is that later one employs fully bayesian approach, it introduces gamma priors on precision parameters of  Note that in contrast to Type II ML which prunes out features at each iteartion Variational Bayes uses all features at each iteration which makes it drastically slower. In original paper (Tipping(2000)) it was argu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from vrvm import VRVR\n",
    "from rvm import SparseBayesianLearner\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As you can see from two examples shown below Variational version of RVR does not provide any visible gain in accuracy of prediction or model sparsity over standard RVR. Nevertheless both RVR and VRVR perform similar to SVR in terms of prediction accuracy on test set, while providing much sparser models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "n = [20,30,40,50,60,70,80,90,100,120,140,160,180,200,300,400,500,1000,1500,2000]\n",
    "n_sim = 20\n",
    "test_train = 0.3\n",
    "\n",
    "\n",
    "def comparison(n):\n",
    "    '''\n",
    "    Comparing RVR, VRVR and SVR \n",
    "    '''\n",
    "    # generate data set\n",
    "    Xc       = np.ones([n,1])\n",
    "    Xc[:,0]  = np.linspace(-5,5,n)\n",
    "    Yc       = 10*np.sinc(Xc[:,0]) + np.random.normal(0,1,n)\n",
    "    X,x,Y,y  = train_test_split(Xc,Yc,test_size = test_train, random_state = 0)\n",
    "\n",
    "    # train rvm with fixed-point optimization\n",
    "    rvr = SparseBayesianLearner(learn_type = \"regression\",method=\"fixed-point\",alpha_max  = 1,\n",
    "                                                                           kernel     = \"gaussian\",\n",
    "                                                                           scaler     = 1)\n",
    "    rvr.fit(X,Y)\n",
    "    y_rvr,var = rvr.predictive_distribution(x)\n",
    "    rvr_err   = mse(y_rvr,y)\n",
    "    rvr_rvs       = np.sum(rvr.active[1:])\n",
    "    \n",
    "    # train variational rvm\n",
    "    vrvr       = VRVR(X,Y,kernel = \"rbf\", scaler = 1, prune_thresh = 1e-2)\n",
    "    vrvr.fit()\n",
    "    y_vrvr,var_vrvr = vrvr.predict_dist(x)\n",
    "    vrvr_err        = mse(y_vrvr,y)\n",
    "    vrvr_rvs        = np.sum(vrvr.active[1:])\n",
    "\n",
    "    # train svr\n",
    "    svr        = GridSearchCV(SVR(kernel = \"rbf\"), param_grid = {'gamma':np.logspace(-2,2,5),\n",
    "                                                             'C':np.logspace(-2,2,5)},\n",
    "                                                   cv = 5)\n",
    "    svr.fit(X,Y)\n",
    "    y_svr      = svr.predict(x)\n",
    "    svr_err    = mse(y_svr,y)\n",
    "    svr_svs    = np.shape(svr.best_estimator_.support_vectors_)[0]\n",
    "    \n",
    "    return {'rvr':[rvr_err,rvr_rvs],'vrvr':[vrvr_err,vrvr_rvs],\n",
    "            'svr':[svr_err,svr_svs]}\n",
    "\n",
    "mse_rvr,mse_svr,mse_vrvr = [],[],[]\n",
    "rvs_rvr,rvs_svr,rvs_vrvr = [],[],[]\n",
    "\n",
    "for n_obs in n:\n",
    "    r_mse,v_mse,s_mse = 0.,0.,0.\n",
    "    r_rvs,v_rvs,s_rvs = 0.,0.,0.\n",
    "    for i in range(n_sim):\n",
    "        result = comparison(n_obs)\n",
    "        r_mse += result['rvr'][0]; r_rvs += result['rvr'][1]\n",
    "        v_mse += result['vrvr'][0]; v_rvs += result['vrvr'][1]\n",
    "        s_mse += result['svr'][0]; s_rvs += result['svr'][1]\n",
    "    mse_rvr.append(r_mse/n_sim); rvs_rvr.append(r_rvs/n_sim)\n",
    "    mse_vrvr.append(v_mse/n_sim); rvs_vrvr.append(v_rvs/n_sim)\n",
    "    mse_svr.append(s_mse/n_sim); rvs_svr.append(s_rvs/n_sim)\n",
    " \n",
    "\n",
    "# comparing accuracy of prediction on test set\n",
    "plt.figure(figsize = (6,9))\n",
    "plt.plot(mse_rvr,\"r-\",label = 'rvm')\n",
    "plt.plot(mse_vrvr,'b-',label = 'vrvm')\n",
    "plt.plot(mse_svr,'g-',label = 'svr')\n",
    "plt.title('N observations vs MSE')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# comparing sparsity of models\n",
    "plt.figure(figsize = (6,9))\n",
    "plt.plot(rvs_rvr,\"r-\",label = 'rvm')\n",
    "plt.plot(rvs_vrvr,'b-',label = 'vrvm')\n",
    "plt.plot(rvs_svr,'g-',label = 'svr')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Sparsity')\n",
    "plt.title('')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston housing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of runs\n",
    "n_sim       = 100 \n",
    "\n",
    "# load data , split training & test\n",
    "boston      = load_boston()\n",
    "XXb,YYb     = scale(boston[\"data\"]),boston[\"target\"]\n",
    "\n",
    "\n",
    "# MSE & Relevant Vectors\n",
    "mse_rvr,mse_vrvr,mse_svr = [],[],[]\n",
    "rvs_rvr,rvs_vrvr,rvs_svr = [],[],[]\n",
    "\n",
    "for i in range(n_sim):\n",
    "    Xb,xb,Yb,yb = train_test_split(XXb,YYb,test_size = 0.7)\n",
    "    \n",
    "    # RVM\n",
    "    rvrb = SparseBayesianLearner(method = \"fixed-point\",learn_type = \"regression\",alpha_max=1e+3,\n",
    "                                                                                  scaler = 10,\n",
    "                                                                                  kernel=\"poly\",\n",
    "                                                                                  p_order = 3)\n",
    "    rvrb.fit(Xb,Yb)\n",
    "    y_rvrb,var  = rvrb.predictive_distribution(xb)\n",
    "    rvrb_err   = mse(y_rvrb,yb)\n",
    "    rvrb_rvs   = np.sum(rvrb.active[1:])\n",
    "    mse_rvr.append(rvrb_err); rvs_rvr.append(rvrb_rvs)\n",
    "\n",
    "    # VRVM\n",
    "    vrvrb            = VRVR(Xb,Yb,kernel = \"poly\", scaler = 10, prune_thresh = 1e-4, order = 3)\n",
    "    vrvrb.fit()\n",
    "    y_vrvr,var_vrvr  = vrvrb.predict_dist(xb)\n",
    "    vrvrb_err        = mse(y_vrvr,yb)\n",
    "    vrvrb_rvs        = np.sum(vrvrb.active[1:])\n",
    "    mse_vrvr.append(vrvrb_err); rvs_vrvr.append(vrvrb_rvs)\n",
    "    \n",
    "    # SVR\n",
    "    svr              = GridSearchCV( SVR(kernel='poly',coef0=1,degree=3),\n",
    "                                     param_grid = {\"gamma\":np.logspace(-2,2,5),\n",
    "                                                   \"C\":np.logspace(-3,3,7)},\n",
    "                                     cv = 5)\n",
    "    svr.fit(Xb,Yb)\n",
    "    y_svr            = svr.predict(xb)\n",
    "    svr_err          = mse(y_svr,yb)\n",
    "    svr_rvs          = np.shape(svr.best_estimator_.support_vectors_)[0]\n",
    "    mse_svr.append(svr_err); rvs_svr.append(svr_rvs)\n",
    "    \n",
    "\n",
    "print \"VRVR median MSE = {0}, median number of RVs = {1}\".format(np.median(mse_vrvr),np.median(rvs_vrvr))\n",
    "print \"RVR median MSE = {0}, median number of RVs = {1}\".format(np.median(mse_rvr),np.median(rvs_rvr))\n",
    "print \"SVR median MSE = {0}, median number of SVs = {1}\".format(np.median(mse_svr),np.median(rvs_svr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comparing prediction accuracy on test set in terms of MSE\n",
    "plt.figure(figsize= (12,8))\n",
    "plt.boxplot((mse_vrvr,mse_rvr,mse_svr))\n",
    "plt.title(\"MSE: VRVR  - vs -   RVR  - vs - SVR\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comparing model sparsity interms of relevant and support vectors\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.boxplot((rvs_vrvr,rvs_rvr,rvs_svr))\n",
    "plt.title(\"MSE: VRVR  - vs -  RVR  - vs - SVR\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.ylim((0,200))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
